# AI技術の活用（ゲームとAI）2026

## 完全版レポート

AI時代の「作り方」と人間の役割

---

# 第1部：導入・問題提起

AIは「便利ツール」から「環境」になった

---

## 第1章：はじめに

### 1.1 この講義の目的

この講義は、AIツールの使い方を教える授業ではない。生成AIが当たり前になった今、本当に問われているのは以下の問いである。

- 人間は何を考えるべきか
- どこに責任が残るのか
- なぜ「基礎」が再び重要になっているのか

本講義では、ゲームという題材を通して、AI時代の思考の構造を整理していく。

### 1.2 2025年から2026年にかけて起きた変化

この1年で、生成AIの立ち位置は大きく変わった。

- 文章を書く
- 画像を作る
- コードを書く

これらが個別の作業ではなく、同時に進行する前提条件になっている。AIはもはや「補助ツール」ではなく、制作そのものを包む環境になりつつある。

### 1.3 それでも成果に差が出る理由

多くの人が同じAIを使っている。しかし、成果には明確な差がある。

- うまく使いこなせる人
- 途中で破綻する人
- 作れた気はするが、何も残らない人

この差は、AIの性能差ではない。

### 1.4 Vibe Coding（バイブコーディング）とは何か

近年よく語られるのがVibe Coding（バイブコーディング）という考え方である。

- 細かい実装よりも雰囲気や意図を自然言語で伝え
- AIに生成させ、人間が監督する

という開発スタイルである。MVPやプロトタイプでは、非常に強力な方法である。

### 1.5 Vibe Codingの魅力

Vibe Codingの利点は明確である。

- 開発速度が圧倒的に速い
- 初学者でも形にできる
- 試行錯誤の回数が増える

「作れない」状態から「とりあえず動く」状態までの距離を劇的に縮めた。

### 1.6 Vibe Codingの落とし穴

一方で、深刻な問題もある。

- AIは論理を理解しているわけではない
- 確率的にそれっぽい出力をしているだけ

以下の場面では破綻が起きやすい：

- 決定論が必要な場面
- 状態が複雑に絡む場面
- 保証が必要な場面

### 1.7 AIは能力ではなく「増幅器」

本講義の最初の結論はこれである。

**AIは能力ではなく、増幅器である。**

- 思考が粗い → 粗さが拡大される
- 思考が精密 → 精密さが加速される

AIは魔法の杖ではない。人間の思考の質をそのまま拡張する装置である。

### 1.8 能力の錯覚（Illusion of Competence）

生成AI時代に特有の問題として、能力の錯覚が起きやすくなっている。

- タスクは完了した
- 動くコードはある
- でも「なぜ動くか」は説明できない

これは理解ではなく、AIが理解を代行しただけの状態である。

### 1.9 理解とパフォーマンスのギャップ

研究でも示されている。

- 作業速度は向上する
- しかし理解度は向上しない

特に初学者ほど、AIの出力を「正解」と誤認しやすくなる。

### 1.10 人間の役割はどこに残るのか

AIは提案できる。しかし、次のことはできない。

- 採用するかどうか決める
- 捨てる判断をする
- 優先順位をつける
- 責任を取る

判断の責任は、必ず人間に残る。

### 1.11 本講義で使うフレームワーク

本講義では、Human × AI を次の3層モデルで整理する。

1. **因数分解**：問題を適切な粒度に分解する
2. **明確化**：目的・制約・仮説を言語化する
3. **判断**：選択・優先順位・責任を引き受ける

このモデルは後半で何度も使う。

---

# 第2部：技術史（ゲームAIは何と戦ってきたか）

複雑性・状態・決定性との闘争史

---

## 第1章：ゲームAIの3つの敵

ゲームAIは誕生以来、3つの根本的な敵と戦い続けてきた。

1. **複雑性（Complexity）**：組み合わせ爆発、探索空間の指数的増大
2. **状態（State）**：ゲーム世界の「今」をどう表現し、追跡するか
3. **決定性（Determinism）**：同じ入力に対して、予測可能で再現可能な振る舞いをどう保証するか

この3つの敵との戦いが、ゲームAI技術の進化を形作ってきた。

---

## 第2章：黎明期（1950-1970年代）

### 2.1 チェスとゲーム理論の始まり

ゲームAIの歴史は、クロード・シャノンの1950年の論文に始まる。

**出典：**
- Shannon, C. E. (1950). "Programming a Computer for Playing Chess." *Philosophical Magazine*, Ser.7, Vol. 41, No. 314.

シャノンは、チェスの探索空間を計算した。

- 合法手の平均：約35手
- 平均ゲーム長：約80手（40手×両プレイヤー）
- 可能な局面数：約10^120（シャノン数）

### 2.2 シャノンの2つの戦略

1. **Type A（力任せ探索）**
   - 全ての可能な手を一定深さまで探索
   - 評価関数で末端ノードを評価
   - ミニマックス法で最善手を選択

2. **Type B（選択的探索）**
   - 有望な手のみを深く探索
   - 人間のような「直感」をアルゴリズム化
   - 現代のモンテカルロ木探索の先駆け

### 2.3 ミニマックス法（Minimax）

相手が最善手を打つと仮定して、自分の最善手を選ぶアルゴリズム。

```
function minimax(node, depth, maximizingPlayer):
    if depth == 0 or node is terminal:
        return evaluate(node)

    if maximizingPlayer:
        value = -∞
        for each child of node:
            value = max(value, minimax(child, depth-1, false))
        return value
    else:
        value = +∞
        for each child of node:
            value = min(value, minimax(child, depth-1, true))
        return value
```

**問題点：** 探索ノード数は O(b^d)（b=分岐数、d=深さ）

### 2.4 アルファベータ枝刈り（Alpha-Beta Pruning）

1958年、アレン・ニューウェルとハーバート・サイモンが発明。

**出典：**
- Knuth, D. E., & Moore, R. W. (1975). "An Analysis of Alpha-Beta Pruning." *Artificial Intelligence*, 6(4), 293-326.

明らかに悪い手を探索しない枝刈り手法。

- α：自分の最低保証スコア
- β：相手の最高許容スコア
- α ≧ β なら枝刈り

**効果：** 最良の場合、探索ノード数を O(b^(d/2)) に削減。

---

## 第3章：アーケードゲームの時代（1970-1980年代）

### 3.1 パックマンのゴーストAI（1980年）

パックマン（Namco, 1980）は、キャラクターごとに異なるAI性格を実装した最初のゲームの一つ。

**出典：**
- Pittman, J. (2011). "The Pac-Man Dossier." *Gamasutra*.

**4体のゴーストの行動アルゴリズム：**

| ゴースト | 日本名 | 性格 | ターゲット計算 |
|---------|--------|------|----------------|
| Blinky | 赤ベイ（おいかけ） | 追跡者 | パックマンの現在位置 |
| Pinky | ピンキー（まちぶせ） | 待ち伏せ | パックマンの4タイル前方 |
| Inky | 青ベイ（きまぐれ） | 気まぐれ | Blinkyとパックマンの位置から計算 |
| Clyde | 愚鈍（おとぼけ） | 愚鈍 | 距離8タイル以上→パックマン、以下→自分の隅 |

### 3.2 Inkyのターゲット計算（最も複雑）

1. パックマンの2タイル前方の位置を取得
2. Blinkyからその位置へのベクトルを計算
3. そのベクトルを2倍に延長した位置がターゲット

結果として、Blinkyと挟み撃ちになる動きが生まれる。単純なルールの組み合わせで複雑な行動を生成する好例。

### 3.3 決定論的AIの利点と限界

**利点：**
- メモリ使用量が極めて少ない（状態を保存する必要がない）
- デバッグが容易（同じ入力→同じ出力）
- プレイヤーが「パターン」を学習できる（攻略の楽しさ）

**限界：**
- 一度パターンを覚えると、ゲームが簡単になりすぎる
- 「知性」を感じない
- 複雑な行動を実装するのが困難

---

## 第4章：有限状態機械の時代（1980-1990年代）

### 4.1 FSM（Finite State Machine）の基礎

**定義：**
FSMは、有限個の状態の集合と、状態間の遷移規則で定義される計算モデル。

**形式的定義：**
- Q：状態の有限集合
- Σ：入力アルファベット（イベント）の有限集合
- δ：遷移関数（Q × Σ → Q）
- q0：初期状態
- F：受理状態の集合（ゲームAIでは通常使用しない）

### 4.2 FSMの実装例（敵キャラクターAI）

```python
class EnemyFSM:
    def __init__(self):
        self.state = "PATROL"

    def update(self, player_distance, health):
        if self.state == "PATROL":
            if player_distance < 10:
                self.state = "CHASE"
            self.patrol()

        elif self.state == "CHASE":
            if player_distance > 15:
                self.state = "PATROL"
            elif player_distance < 2:
                self.state = "ATTACK"
            elif health < 20:
                self.state = "FLEE"
            self.chase_player()

        elif self.state == "ATTACK":
            if player_distance > 2:
                self.state = "CHASE"
            elif health < 20:
                self.state = "FLEE"
            self.attack()

        elif self.state == "FLEE":
            if health > 50:
                self.state = "PATROL"
            self.flee()
```

### 4.3 FSMの状態爆発問題

**問題：** 状態数と遷移数が指数的に増加する。

例：3つの独立した条件（敵の視認、体力、弾薬）を考慮する場合

- 単純FSM：2^3 = 8状態が必要
- 5つの条件：2^5 = 32状態
- 10の条件：2^10 = 1024状態

**Half-Life（1998）の敵AI：**
Valveの開発者は、FSMの限界に直面した。兵士AIに50以上の状態が必要になり、遷移の組み合わせが管理不能になった。

**出典：**
- Isla, D. (2005). "Handling Complexity in the Halo 2 AI." *GDC 2005*.

### 4.4 階層型FSM（HFSM）

**解決策：** 状態をネストして階層化する。

```
[Combat]
├── [Aggressive]
│   ├── [Charge]
│   └── [Attack]
└── [Defensive]
    ├── [TakeCover]
    └── [Retreat]

[NonCombat]
├── [Patrol]
└── [Investigate]
```

**利点：**
- 関連する状態をグループ化
- 上位状態の遷移で、下位状態全体を切り替え可能
- コードの再利用性向上

**限界：**
- 依然として状態爆発の根本解決にはならない
- 並列行動の表現が困難

---

## 第5章：ビヘイビアツリーの登場（2000年代）

### 5.1 Halo 2と行動木の発明（2004年）

**背景：**
Halo 2（Bungie, 2004）の開発中、AIプログラマーのDamian Islaは、FSMの限界を打破する新しいアーキテクチャを考案した。

**出典：**
- Isla, D. (2005). "Handling Complexity in the Halo 2 AI." *GDC 2005*.
- Champandard, A. J. (2007). "Understanding Behavior Trees." *AiGameDev.com*.

### 5.2 ビヘイビアツリー（Behavior Tree）の構造

```
        [Selector]
       /    |    \
  [Sequence] [Sequence] [Action]
   /    \     /    \      Patrol
Attack Flee Cover Shoot
```

**ノードタイプ：**

1. **Composite（複合ノード）**
   - **Selector（選択）**：子ノードを順に評価し、最初に成功したものを実行
   - **Sequence（順序）**：子ノードを順に実行し、全て成功で成功
   - **Parallel（並列）**：複数の子ノードを同時実行

2. **Decorator（装飾ノード）**
   - **Inverter**：子の結果を反転
   - **Repeater**：子を指定回数繰り返す
   - **Succeeder**：常に成功を返す

3. **Leaf（葉ノード）**
   - **Action**：実際の行動を実行
   - **Condition**：条件をチェック

### 5.3 ビヘイビアツリーの実装例

```python
class Node:
    def tick(self):
        raise NotImplementedError

class Selector(Node):
    def __init__(self, children):
        self.children = children

    def tick(self):
        for child in self.children:
            result = child.tick()
            if result == SUCCESS:
                return SUCCESS
            if result == RUNNING:
                return RUNNING
        return FAILURE

class Sequence(Node):
    def __init__(self, children):
        self.children = children

    def tick(self):
        for child in self.children:
            result = child.tick()
            if result == FAILURE:
                return FAILURE
            if result == RUNNING:
                return RUNNING
        return SUCCESS
```

### 5.4 ビヘイビアツリーがFSMを超える理由

| 観点 | FSM | Behavior Tree |
|------|-----|---------------|
| 状態の追加 | 全遷移を再検討 | ノード追加のみ |
| 並列行動 | 困難 | Parallelノードで容易 |
| 再利用性 | 低い | サブツリーとして再利用可能 |
| 可読性 | 状態図が複雑化 | 階層構造で明確 |
| デバッグ | 状態遷移の追跡が困難 | 実行パスが視覚化可能 |

### 5.5 Unreal Engine 4への統合

2014年、Epic GamesはUnreal Engine 4にビヘイビアツリーを標準搭載。

**UE4のBT拡張機能：**
- **Blackboard**：AIの知識データベース
- **Service**：定期的に実行されるバックグラウンドタスク
- **Observer Aborts**：条件変化時の即座の中断

---

## 第6章：計画型AI（2000-2010年代）

### 6.1 GOAP（Goal-Oriented Action Planning）

**起源：**
F.E.A.R.（Monolith Productions, 2005）で初めて商用ゲームに実装。

**開発者：** Jeff Orkin（MIT Media Lab出身）

**出典：**
- Orkin, J. (2003). "Applying Goal-Oriented Action Planning to Games." *AI Game Programming Wisdom 2*.
- Orkin, J. (2006). "Three States and a Plan: The A.I. of F.E.A.R." *GDC 2006*.

### 6.2 GOAPの仕組み

1. **World State（世界状態）**：現在の状態をKey-Valueで表現
2. **Goal（目標）**：達成したい状態
3. **Action（行動）**：前提条件と効果を持つ
4. **Planner（計画器）**：A*などで行動列を探索

**例：敵を倒すという目標**

```
現在の世界状態:
{
  "enemy_alive": true,
  "weapon_loaded": false,
  "ammo_count": 10,
  "in_cover": false
}

目標:
{
  "enemy_alive": false
}

利用可能な行動:
- Reload: 前提{ammo_count > 0}, 効果{weapon_loaded: true}
- Shoot: 前提{weapon_loaded: true}, 効果{enemy_alive: false}
- TakeCover: 前提{}, 効果{in_cover: true}

計画結果: [Reload] → [Shoot]
```

### 6.3 F.E.A.R.のAIが革命的だった理由

**従来のAI（スクリプト型）：**
```
IF player_visible AND health > 50 THEN attack
ELSE IF player_visible AND health <= 50 THEN take_cover
ELSE patrol
```

**F.E.A.R.のGOAP AI：**
- 目標「敵を排除する」を与えるだけ
- AIが自律的に「カバーを取る→リロード→射撃」を計画
- 状況に応じて計画を動的に変更

**結果：**
- 兵士AIが「賢く」見える行動を自然に生成
- フランキング（側面攻撃）、制圧射撃、連携行動
- 開発者は行動の組み合わせを全て記述する必要がない

### 6.4 HTN（Hierarchical Task Network）

**GOAPの限界：**
- 単純な行動のみで複雑なタスクを表現しにくい
- 行動の順序に関する知識を活かしにくい

**HTN（階層型タスクネットワーク）：**

```
高レベルタスク: 敵を倒す
├── 方法1: 近接攻撃
│   ├── 接近する
│   └── 殴る
├── 方法2: 射撃
│   ├── 武器を構える
│   ├── 照準を合わせる
│   └── 撃つ
└── 方法3: 爆発物使用
    ├── グレネードを取り出す
    ├── 投げる
    └── 退避する
```

**出典：**
- Hoang, H., Lee-Urban, S., & Muñoz-Avila, H. (2005). "Hierarchical Plan Representations for Encoding Strategic Game AI." *AIIDE 2005*.

---

## 第7章：空間認識と経路探索

### 7.1 A*アルゴリズム（1968年）

**発明者：** Peter Hart, Nils Nilsson, Bertram Raphael（スタンフォード研究所）

**出典：**
- Hart, P. E., Nilsson, N. J., & Raphael, B. (1968). "A Formal Basis for the Heuristic Determination of Minimum Cost Paths." *IEEE Transactions on Systems Science and Cybernetics*, 4(2), 100-107.

**アルゴリズム：**

```python
def a_star(start, goal, graph, heuristic):
    open_set = [(0, start)]
    came_from = {}
    g_score = {start: 0}
    f_score = {start: heuristic(start, goal)}

    while open_set:
        current = heapq.heappop(open_set)[1]

        if current == goal:
            return reconstruct_path(came_from, current)

        for neighbor in graph.neighbors(current):
            tentative_g = g_score[current] + graph.cost(current, neighbor)

            if tentative_g < g_score.get(neighbor, float('inf')):
                came_from[neighbor] = current
                g_score[neighbor] = tentative_g
                f_score[neighbor] = tentative_g + heuristic(neighbor, goal)
                heapq.heappush(open_set, (f_score[neighbor], neighbor))

    return None  # パスなし
```

**f(n) = g(n) + h(n)**
- g(n)：開始点からnまでの実コスト
- h(n)：nから目標までのヒューリスティック（推定コスト）
- h(n)が許容的（実際のコスト以下）なら最適解を保証

### 7.2 ナビゲーションメッシュ（NavMesh）

**従来のウェイポイント方式の問題：**
- 手動配置が大変
- 経路が不自然（直線的）
- 動的な障害物への対応が困難

**NavMeshの原理：**
- 歩行可能な領域を凸多角形で分割
- 多角形の隣接関係でグラフを構築
- A*で多角形間の経路を探索

**Recast/Detour（Mikko Mononen, 2009）：**
オープンソースのNavMesh生成・経路探索ライブラリ。Unity、Unreal Engine、多くのAAAゲームで採用。

### 7.3 影響マップ（Influence Map）

**用途：** 戦術的な位置評価

```python
def calculate_influence_map(width, height, units):
    influence = [[0.0] * width for _ in range(height)]

    for unit in units:
        for y in range(height):
            for x in range(width):
                distance = math.sqrt((x - unit.x)**2 + (y - unit.y)**2)
                inf = unit.strength / (1 + distance * 0.5)
                if unit.team == ENEMY:
                    influence[y][x] -= inf
                else:
                    influence[y][x] += inf

    return influence
```

**応用例：**
- 正の値：味方支配領域（安全）
- 負の値：敵支配領域（危険）
- カバー位置の選択、フランキング経路の計算に使用

**出典：**
- Tozour, P. (2001). "Influence Mapping." *Game Programming Gems 2*.

---

## 第8章：強化学習の時代（2010年代-現在）

### 8.1 Deep Q-Network（DQN, 2013-2015）

**DeepMindのブレークスルー：**
Atari 2600のゲームを、画面ピクセルのみから学習。

**出典：**
- Mnih, V., et al. (2015). "Human-level control through deep reinforcement learning." *Nature*, 518(7540), 529-533.

**アーキテクチャ：**
```
入力: 84x84x4 (4フレーム分のグレースケール画像)
    ↓
Conv1: 32フィルタ, 8x8, stride 4
    ↓
Conv2: 64フィルタ, 4x4, stride 2
    ↓
Conv3: 64フィルタ, 3x3, stride 1
    ↓
全結合: 512ユニット
    ↓
出力: 各行動のQ値
```

**技術的革新：**
- **Experience Replay**：過去の経験をバッファに保存し、ランダムサンプリングで学習
- **Target Network**：Q値の更新対象と評価対象を分離し、学習を安定化

### 8.2 AlphaGo / AlphaZero（2016-2017）

**AlphaGo（2016）：**
- モンテカルロ木探索 + 深層学習
- 人間の棋譜で事前学習 + 自己対戦で強化学習

**AlphaZero（2017）：**
- 人間の知識を一切使わない
- ルールのみから自己対戦で学習
- チェス、将棋、囲碁で人間とAlphaGoを超える

**出典：**
- Silver, D., et al. (2016). "Mastering the game of Go with deep neural networks and tree search." *Nature*, 529(7587), 484-489.
- Silver, D., et al. (2017). "Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm." *arXiv:1712.01815*.

### 8.3 OpenAI Five（2018-2019）

**Dota 2での成果：**
- 5v5のチーム戦を強化学習で学習
- 約45,000年分の自己対戦（分散計算）
- 世界チャンピオンチームに勝利

**技術的特徴：**
- LSTM（長期記憶）の使用
- PPO（Proximal Policy Optimization）アルゴリズム
- 1万以上の行動空間
- 約2万の観測変数

**出典：**
- Berner, C., et al. (2019). "Dota 2 with Large Scale Deep Reinforcement Learning." *arXiv:1912.06680*.

### 8.4 強化学習の商用ゲームでの現実

**なぜAAAゲームでは強化学習が普及しないのか：**

1. **学習コスト**
   - 数千〜数百万時間の学習時間
   - 大規模な計算リソースが必要

2. **再現性の問題**
   - 学習結果がランダムシードに依存
   - 同じ学習を再現できない

3. **説明可能性の欠如**
   - なぜその行動を選んだか説明できない
   - バグの原因特定が困難
   - QA（品質保証）が困難

4. **バランス調整の困難さ**
   - 「少しだけ弱くする」が困難
   - 難易度調整がパラメータ調整ではなく再学習

**現実的な適用領域：**
- NPCの動作生成（アニメーション遷移）
- 自動ゲームテスト
- ゲームバランス調整の補助

---

## 第9章：象徴的事例研究

### 9.1 Halo 2のエンカウンターデザイン（2004）

Bungieは、AIの「賢さ」だけでなく「面白さ」を重視した。

**3つの原則：**
1. **反応的であれ**：プレイヤーの行動に即座に反応
2. **予測可能であれ**：プレイヤーが「読める」行動
3. **間違いを犯せ**：完璧すぎるAIは面白くない

**出典：**
- Butcher, C., & Griesemer, J. (2002). "The Illusion of Intelligence: The Integration of AI and Level Design in Halo." *GDC 2002*.

### 9.2 Left 4 Dead のAI Director（2008）

**概要：**
プレイヤーの状況に応じて、ゲームのペースを動的に調整。

**Intensity（緊張度）のモデル：**
```
緊張度の更新:
- 敵との戦闘中: 緊張度 ↑
- 安全地帯: 緊張度 ↓ (時間経過)
- 仲間がダウン: 緊張度 ↑↑

スポーン制御:
- 緊張度が高い: スポーン抑制、回復アイテム増加
- 緊張度が低い: 大群スポーン、特殊感染者投入
```

**Peak-Valley設計：**
緊張のピークと谷を交互に配置し、心理学的な「恐怖→安堵」のサイクルを生み出す。

**出典：**
- Booth, M. (2009). "The AI Systems of Left 4 Dead." *AI Game Programming Wisdom 4*.

### 9.3 麻雀AI Suphx（Microsoft, 2019）

**不完全情報ゲームへの挑戦：**

麻雀は、典型的な不完全情報ゲーム。
- 相手の手牌が見えない
- 山牌の内容が不明
- 他家の待ち・鳴き意図が不明

**Suphxの技術：**
1. **Oracle Guiding**：完全情報を知っている「神」の行動を模倣学習
2. **Self-Play**：自己対戦による強化学習
3. **多視点学習**：4人のプレイヤー全ての視点で学習

**成果：**
天鳳（オンライン麻雀）で十段を達成。人間のトッププレイヤーを超える成績。

**出典：**
- Li, J., et al. (2020). "Suphx: Mastering Mahjong with Deep Reinforcement Learning." *arXiv:2003.13590*.

---

## 第10章：LLM時代のゲームAI（2023年-）

### 10.1 LLMの可能性

**NPCとの自然言語対話：**
- Character.ai、Inworld AI
- 事前定義されたセリフからの脱却
- プレイヤーの自由な質問に応答

**動的なコンテンツ生成：**
- ストーリー分岐の自動生成
- クエストの動的作成
- 世界設定の即興生成

### 10.2 LLMの限界

**ゲームAIとしての根本的問題：**

1. **レイテンシ**：応答に数百ミリ秒〜数秒。60FPSのゲームでは16.7ms以内に判断が必要。

2. **決定論性の欠如**：同じ入力に異なる出力。再現可能なデバッグが困難。

3. **状態管理の弱さ**：長期的な一貫性の維持が困難。「3ターン前に約束したこと」を忘れる。

4. **幻覚（Hallucination）**：存在しないアイテムや能力を参照。ゲームルールの逸脱。

### 10.3 ハイブリッドアプローチ

**現実的な解決策：**

```
[LLM]（創造性・自然言語）
    ↓
[ルールベースフィルター]（安全性・一貫性）
    ↓
[ゲーム状態管理]（決定論・検証可能性）
    ↓
[実行]
```

LLMは「脳」、ルールベースは「身体」として設計する。

---

## 第11章：技術史の教訓

### 11.1 繰り返されるパターン

ゲームAIの歴史は、同じパターンを繰り返している：

```
新技術の登場
    ↓
「万能だ！」という期待
    ↓
現実の制約との衝突
    ↓
適用領域の限定
    ↓
他技術とのハイブリッド化
    ↓
成熟した実用的手法へ
```

### 11.2 変わらない3つの敵

1. **複雑性**：FSM → Behavior Tree → GOAP → HTN。複雑性を「管理」する方法が進化。

2. **状態**：ワールドモデルの精緻化。Blackboard、影響マップ。しかし「完全な世界モデル」は常に不可能。

3. **決定性**：学習型AIは決定性と相性が悪い。ゲームでは再現可能性が品質保証に直結。ルールベースと学習型のハイブリッドが現実解。

### 11.3 人間の役割

**AIが進化しても変わらないこと：**

- どのAI技術を「選択」するかは人間の判断
- AIの行動が「面白いか」を評価するのは人間
- バグの原因を「特定」するのは人間
- AIの失敗の「責任」を取るのは人間

---

# 第3部：生成AI・LLMの位置づけ

何が新しく、何が変わらないか

---

## 第1章：生成AIの技術的基盤

### 1.1 Transformerアーキテクチャ（2017年）

**出典：**
- Vaswani et al. (2017). "Attention Is All You Need."

Google Brainの研究チームが発表。従来のRNN/LSTMの限界を突破し、並列計算を可能にした。

### 1.2 Self-Attentionの仕組み

- Query, Key, Valueの3つのベクトル
- 各トークンが他の全トークンとの関連度を計算
- Attention(Q,K,V) = softmax(QK^T / √d_k) V
- 「文脈」を数学的に捉える手法

### 1.3 なぜTransformerが革新的だったか

- **RNNの問題**：逐次処理（並列化困難）
- **LSTMの問題**：長距離依存の学習が困難
- **Transformerの解決策**：
  - 全てのトークン間の関係を一度に計算
  - 位置エンコーディングで順序を保持
  - GPUの並列計算能力を最大活用

### 1.4 スケーリング則（Scaling Laws）

**出典：**
- Kaplan et al. (2020). "Scaling Laws for Neural Language Models."

**発見：** モデルサイズ、データ量、計算量を増やすと予測可能に性能向上。パラメータ数10倍で性能は一定比率で向上。

### 1.5 GPTシリーズの進化

| モデル | 年 | パラメータ数 |
|--------|-----|-------------|
| GPT-1 | 2018 | 1.17億 |
| GPT-2 | 2019 | 15億 |
| GPT-3 | 2020 | 1750億 |
| GPT-4 | 2023 | 推定1兆以上（非公開） |

パラメータ数は4年で1万倍以上に増加。

---

## 第2章：LLMは何ができて何ができないか

### 2.1 LLMの本質：確率的言語モデル

- 入力：トークン列 [t1, t2, ..., tn]
- 出力：次のトークンの確率分布 P(t_{n+1} | t1, ..., tn)
- 「最も確率の高い続き」を生成
- これは「理解」ではなく「予測」

### 2.2 In-Context Learning（文脈内学習）

**出典：**
- Brown et al. (2020). "Language Models are Few-Shot Learners."

- Few-shot learning：数例を見せるだけで新しいタスクに対応
- 従来のML：タスクごとに再学習が必要
- LLM：プロンプトに例を含めるだけ

### 2.3 Chain-of-Thought（思考の連鎖）

**出典：**
- Wei et al. (2022). "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models."

「ステップバイステップで考えて」と指示すると推論精度が向上。中間ステップの生成が推論を改善する。

### 2.4 LLMが得意なこと

- 自然言語の生成・変換・要約
- パターンマッチングと類推
- 知識の検索と統合
- 文脈に応じた応答生成
- コード生成（自然言語→プログラム）

### 2.5 LLMが苦手なこと

- 厳密な論理推論
- 数学的計算（特に多桁の計算）
- 最新情報へのアクセス
- 長期的な一貫性の維持
- 「知らない」ことを知ること（メタ認知）

### 2.6 幻覚（Hallucination）問題

**定義：** 事実と異なる情報をもっともらしく生成する現象。

**原因：** 確率的生成の本質的性質。LLMは「正しいか」ではなく「ありそうか」で出力。

**具体例：**
- 「アインシュタインの1987年の論文では...」（1955年に死去）
- 存在しないAPIやライブラリの提案
- 架空の法律や規則の引用

---

## 第3章：生成AIとゲームAIの比較

### 3.1 根本的な設計思想の違い

| 観点 | ゲームAI | 生成AI |
|------|----------|--------|
| 設計思想 | 決定論的、再現可能、検証可能 | 確率的、創造的、予測不能 |
| 優先事項 | 「正しく動く」 | 「それらしく見える」 |
| レイテンシ | 16.7ms以内（60FPS） | 100ms〜数秒 |
| 状態管理 | 明示的（FSM、BT、Blackboard） | 暗黙的（コンテキストウィンドウ） |
| エラー | 再現可能、修正可能 | 確率的、同じ入力で異なるエラー |
| 責任 | 開発者がロジックを書く→開発者の責任 | モデルが出力→責任の所在が曖昧 |

---

## 第4章：Vibe Codingの功罪

### 4.1 Vibe Codingとは何か

**提唱者：** Andrej Karpathy（元Tesla AI責任者、OpenAI創設メンバー）

**定義：** AIに自然言語で指示し、生成されたコードを「監督」するスタイル。細かい実装詳細より「雰囲気」や「意図」を伝える。

### 4.2 Vibe Codingの適用領域

- プロトタイプ作成（MVP）
- 使い捨てスクリプト
- 学習・実験目的のコード
- 個人プロジェクト
- 「動けばいい」場面

### 4.3 Vibe Codingの危険性

- コードを理解していない状態での「完成」
- 隠れたバグの蓄積
- セキュリティ脆弱性の見落とし
- 技術的負債の急速な蓄積
- 「なぜ動くか分からない」コードの量産

### 4.4 能力の錯覚（Illusion of Competence）

**出典：**
- Roediger & Karpicke (2006)

**定義：** 実際には理解していないのに理解したと思い込む現象。

LLM時代に特に顕著。タスクは完了し、コードは動くが、「なぜ動くか」は説明できない。

### 4.5 研究：AIツールと学習効果

**出典：**
- Bastani et al. (2024). "Generative AI Can Harm Learning."

**発見：** AIチューターを使った学生は、使わなかった学生より成績が低下。AIが「考える」プロセスを代行してしまうことが原因。

### 4.6 生成的負債（Generative Debt）

技術的負債（Technical Debt）の拡張概念。

**定義：** 理解なく生成されたコードが蓄積する問題。

**特徴：**
- 負債の存在に気づきにくい
- 問題が表面化するまで時間がかかる
- 返済（修正）に元の生成より多くの時間がかかる

---

## 第5章：何が新しく、何が変わらないか

### 5.1 新しいこと

1. **自然言語インターフェース**：コードを書かずにプログラムを作れる可能性
2. **Few-shot適応**：数例を見せるだけで新しいタスクに対応
3. **創造的補助**：アイデアの壁打ち相手、多様な選択肢の提示

### 5.2 変わらないこと

1. **決定論性の必要性**：ゲーム、金融、医療では再現可能性が必須
2. **状態管理の重要性**：複雑なシステムは状態を追跡する必要がある
3. **検証の必要性**：AIが生成したコードも検証が必要
4. **人間の判断**：採用、優先順位、責任は人間が行う
5. **基礎の重要性**：AIを使いこなすには基礎知識が必要

---

## 第6章：ゲーム開発における生成AIの現実的な活用

### 6.1 In-Game AI（ゲーム内AI）

- NPCとの自然言語対話
- 動的なストーリー生成
- プレイヤー行動への適応的応答
- ただし、レイテンシと一貫性の問題がある

### 6.2 Out-Game AI（ゲーム外AI）

- アセット生成（テクスチャ、3Dモデル、音楽）
- シナリオ・ダイアログの草案作成
- バグレポートの分析
- ドキュメント生成

### 6.3 ハイブリッドアーキテクチャ

```
[LLM]（創造性・柔軟性）
    ↓
[ルールベース]（安全性・一貫性）
    ↓
[状態管理システム]（整合性・追跡可能性）
```

3層構造での活用が現実解。

---

## 第7章：まとめ

### 7.1 生成AIは「ツール」であり「増幅器」

- 万能ではない
- 適用領域がある
- 思考が粗い → 粗さが拡大
- 思考が精密 → 精密さが加速

### 7.2 ゲームAI技術は消えない

- 決定論的AIは依然として必要
- FSM、BT、GOAPは現役
- LLMは「追加」であり「置換」ではない

### 7.3 基礎はより重要になる

- AIを評価するには基礎知識が必要
- デバッグ能力の重要性は増す
- 批判的思考（Critical Thinking）が必須

---

# 第4部：人間×AIの設計原則（方法論）

因数分解・明確化・判断

---

## 第1章：因数分解（Factorization）

### 1.1 因数分解とは何か

- **数学の因数分解**：12 = 2 × 2 × 3
- **問題解決での因数分解**：複雑な問題を扱いやすい部分に分解
- **AI活用での因数分解**：AIに渡す前に問題を適切なサイズに切る

### 1.2 なぜ因数分解が重要か

AIは「大きすぎる問題」が苦手。

- コンテキストウィンドウの制限
- 複合的な指示での混乱
- 曖昧な目標での発散

人間が切り分けることで、AIの能力を最大化できる。

### 1.3 因数分解の粒度

- **粗すぎる**：「ゲームを作って」→ 何を作るか不明
- **細かすぎる**：「この変数をintからfloatに変えて」→ 文脈が見えない
- **適切な粒度**：「プレイヤーの移動処理を実装して」→ スコープが明確

### 1.4 因数分解の3つのレベル

1. **戦略レベル**：何を達成するか（Why）
2. **戦術レベル**：どうアプローチするか（How）
3. **実装レベル**：具体的に何をするか（What）

### 1.5 AIに適した因数分解

- 一つのタスクが一つの目的を持つ
- 入力と出力が明確
- 他のタスクとの依存関係が少ない
- 検証可能な完了条件がある

### 1.6 因数分解のアンチパターン

- 「全部やって」：スコープが無限
- 「いい感じに」：基準が不明
- 「〜など」：範囲が曖昧
- 「必要に応じて」：判断基準が不明

---

## 第2章：明確化（Clarification）

### 2.1 明確化とは何か

- 暗黙の前提を言語化する
- 目的を具体的に記述する
- 制約条件を列挙する
- 成功基準を定義する

### 2.2 明確化の5つの要素

1. **目的（Purpose）**：なぜこれをするのか
2. **入力（Input）**：何を与えられるか
3. **出力（Output）**：何を得たいか
4. **制約（Constraints）**：何を守らなければならないか
5. **仮説（Hypothesis）**：何を前提としているか

### 2.3 明確化の例

**悪い例：**「コードを改善して」

**良い例：**「このコードのパフォーマンスを改善して。現在1000件の処理に5秒かかっているが、1秒以内にしたい」

### 2.4 明確化のテンプレート

```
目的：[この作業で達成したいこと]
入力：[与えられるデータ・情報]
出力：[期待する成果物の形式]
制約：[技術的・ビジネス的な制限]
仮説：[前提としている条件]
```

---

## 第3章：判断（Judgment）

### 3.1 判断とは何か

- 複数の選択肢から一つを選ぶ
- 優先順位をつける
- トレードオフを評価する
- 責任を引き受ける

AIは「提案」できるが「決定」はできない。

### 3.2 なぜ判断が人間に残るか

- AIは「良い/悪い」の基準を持たない
- 確率的に「ありそう」を出力するだけ
- 価値判断は人間の領域
- 責任は人間にしか帰属しない

### 3.3 判断の3つのレベル

1. **採用判断**：AIの出力を使うかどうか
2. **優先順位判断**：何を先にやるか
3. **責任判断**：誰が責任を取るか

### 3.4 採用判断の基準

- **正しいか（Correctness）**：論理的に正しいか
- **適切か（Appropriateness）**：目的に合っているか
- **効率的か（Efficiency）**：無駄がないか
- **安全か（Safety）**：リスクがないか

### 3.5 判断のアンチパターン

- **丸投げ**：「AIが言ったから」で終わる
- **思考停止**：選択肢を比較しない
- **責任転嫁**：「AIのせい」にする
- **過信**：AIの出力を検証しない

---

## 第4章：3層モデルの統合

### 4.1 3層モデルの全体像

```
入力 → [因数分解] → サブタスク群
サブタスク → [明確化] → 明確な仕様
明確な仕様 → [AI処理] → 出力候補群
出力候補 → [判断] → 最終成果物
```

Human → AI → Human のサンドイッチ構造。

### 4.2 AIに使われる人 vs 使いこなす人

**AIに使われる人：**
- 因数分解しない：「全部やって」と丸投げ
- 明確化しない：「いい感じに」で済ませる
- 判断しない：AIの出力をそのまま採用

**AIを使いこなす人：**
- 因数分解する：問題を適切なサイズに切る
- 明確化する：仕様を具体的に伝える
- 判断する：出力を評価し、選択する

### 4.3 能力差が拡大する理由

AIは「増幅器」である。

- 思考が粗い人 → 粗さが拡大
- 思考が精密な人 → 精密さが加速

AI時代は「思考の質」がより重要になる。

---

# 第5部：制作・産業への接続

工程・QA・運用・責任

---

## 第1章：ゲーム開発の工程

### 1.1 開発モデル

**ウォーターフォール型：**
企画 → 設計 → 実装 → テスト → リリース

**アジャイル型：**
短いサイクル（スプリント）で反復。常にプレイ可能な状態を維持。

### 1.2 開発フェーズ

1. **プリプロダクション**：コンセプト策定、プロトタイプ作成、技術検証
2. **プロダクション**：本格的な開発、アセット制作、プログラム実装
3. **ポストプロダクション**：バグ修正、最適化、ローカライズ
4. **ライブオペレーション**：リリース後の継続的運用

### 1.3 AI時代の工程変化

| 工程 | AI活用度 | 内容 |
|------|----------|------|
| 企画 | 高 | アイデア出し、壁打ち |
| 設計 | 中 | ドキュメント生成、レビュー |
| 実装 | 高 | コード生成、補完 |
| テスト | 高 | 自動テスト、網羅性向上 |
| 運用 | 中 | 分析、レポート生成 |

---

## 第2章：品質保証（QA）

### 2.1 QAの種類

- **機能テスト**：仕様通りに動くか
- **互換性テスト**：様々な環境で動くか
- **パフォーマンステスト**：性能要件を満たすか
- **ユーザビリティテスト**：使いやすいか

### 2.2 テストピラミッド

```
       E2Eテスト（少）
      /           \
   結合テスト（中）
  /                 \
単体テスト（多・速い）
```

下層ほど多く、上層ほど少なく。

### 2.3 AIによるQAの進化

- 自動プレイテスト
- 異常検知
- バグの自動分類
- テストケース生成

### 2.4 自動テストの限界

**自動化できるもの：** 機能の正常動作、パフォーマンス計測

**自動化できないもの：** 「楽しいか」「気持ちいいか」「世界観に合っているか」

### 2.5 QAエンジニアの役割変化

- 従来：手動でバグを見つける
- 現在：自動テストを設計・監督する
- 今後：AIテストの結果を評価・判断する

「テスター」から「品質設計者」へ。

---

## 第3章：セキュリティと責任

### 3.1 ゲームセキュリティの重要性

- 課金システムの保護
- 個人情報の保護
- チート対策
- 不正アクセス防止

### 3.2 生成AIとセキュリティリスク

**出典：**
- Pearce et al. (2022). "Asleep at the Keyboard"

**発見：** GitHub Copilotが生成したコードの40%に脆弱性。

脆弱性の種類：
- SQLインジェクション
- クロスサイトスクリプティング（XSS）
- 認証・認可の不備

### 3.3 責任の所在

AIが書いたコードで事故が起きたら、法的責任は「使った人・組織」に帰属。

「AIが書いた」は免責にならない。だから人間のレビューが必須。

---

## 第4章：運用とライブサービス

### 4.1 Games as a Service（GaaS）

ゲームを「製品」ではなく「サービス」として提供。

- 継続的なアップデート
- 定期的なイベント
- 長期的な収益化

### 4.2 監視と可観測性

- **メトリクス**：数値データ（CPU使用率、レスポンス時間）
- **ログ**：イベントの記録
- **トレース**：リクエストの追跡

### 4.3 データ分析

KPI（重要業績評価指標）：
- DAU（日間アクティブユーザー）
- MAU（月間アクティブユーザー）
- ARPU（ユーザーあたり収益）
- LTV（生涯価値）
- 継続率

---

## 第5章：産業構造の変化

### 5.1 新しい職種

- AI/MLエンジニア
- プロンプトエンジニア
- AIトレーナー
- 倫理・ガバナンス担当

### 5.2 求められるスキルの変化

- 従来：専門スキルの深さ
- 現在：複数スキルの組み合わせ
- 今後：AI活用能力 + 判断力

「T字型人材」から「π型人材」へ。

### 5.3 キャリアパス

**プログラマーの専門分野：**
- ゲームプレイプログラマー
- グラフィックスプログラマー
- AIプログラマー
- ネットワークプログラマー
- サーバーエンジニア

---

# 第6部：教育への還元

学び方・演習・進路

---

## 第1章：AI時代の学習原則

### 1.1 Productive Struggle（生産的な苦闘）

**出典：**
- Kapur (2008). "Productive Failure"

すぐに答えを見ない。自分で考え、試行錯誤する。その過程が理解を生む。

### 1.2 なぜ「苦闘」が必要か

**出典：**
- Bjork (1994). "Desirable Difficulties"

脳は「楽」を好む。しかし「楽」では学習が起きない。認知的負荷が学習を促進。

### 1.3 Human → AI → Human モデル

1. **Human（前）**：自分で考える、設計する
2. **AI（中）**：生成、補助、比較
3. **Human（後）**：検証、編集、判断

AIを「真ん中」に置く。

### 1.4 AIは「家庭教師」であるべき

- 宿題を代行する存在ではない
- 答え合わせをする存在として使う
- 自分で解いてから、確認に使う
- 順番を間違えると学習が崩れる

### 1.5 理解が生まれる瞬間

- 説明を聞いた瞬間ではない
- 正解を見た瞬間でもない
- 「なぜ動かないか分からない」時間
- 「なぜ動いたか」を発見した瞬間

---

## 第2章：基礎の重要性

### 2.1 プログラミングの基礎

- 変数、型、演算子
- 制御構造（if、for、while）
- 関数、クラス
- データ構造（配列、リスト、辞書）

### 2.2 アルゴリズムの基礎

- 計算量（O記法）
- ソート、探索
- 再帰
- グラフ探索（BFS、DFS）
- 動的計画法

### 2.3 数学の基礎

- 線形代数（ベクトル、行列）
- 確率・統計
- 三角関数
- 微分・積分（基礎）

### 2.4 デバッグ能力

デバッグとは、エラーを直す技術ではなく、原因を切り分ける思考。

**プロセス：**
1. 症状を正確に把握する
2. 再現条件を特定する
3. 原因の仮説を立てる
4. 仮説を検証する
5. 原因を特定し修正する
6. 副作用がないか確認する

### 2.5 読む力（コードリーディング）

- 他人のコードを読む能力
- AIが生成したコードを読む能力
- 読めないコードは使えない

---

## 第3章：実践的な学習方法

### 3.1 プロジェクトベース学習

- 実際に作ることで学ぶ
- 「完成させる」経験が重要
- 小さく始めて、完成させる
- 完璧より完成

### 3.2 ゲームジャムへの参加

- Global Game Jam（48時間）
- Ludum Dare（48-72時間）
- unity1week（1週間）

強制的に「完成」を経験できる。

### 3.3 アウトプット駆動学習

- 学んだことを発信する
- ブログ、Qiita、Zenn
- 説明することで理解が深まる

### 3.4 継続の仕組み化

- 毎日少しずつ
- 習慣化する
- 記録をつける
- 完璧を求めない

---

## 第4章：演習課題

### 4.1 演習レベル

- Level 1：写経・模倣
- Level 2：変更・拡張
- Level 3：設計・実装
- Level 4：評価・改善

### 4.2 演習1：FSMの実装（Level 1-2）

**課題：** 敵キャラクターのFSMを実装。状態はPatrol、Chase、Attack、Flee。

**学習目標：** FSMの基本構造、状態遷移の概念、コードでの表現

### 4.3 演習2：ビヘイビアツリーの実装（Level 2-3）

**課題：** シンプルなBTフレームワークを実装。Selector、Sequence、Actionノード。

**学習目標：** BTの構造、木構造のプログラミング、FSMとの違い

### 4.4 演習3：A*経路探索（Level 2-3）

**課題：** A*アルゴリズムを実装。グリッドマップでの経路探索。

**学習目標：** グラフ探索、ヒューリスティック、データ構造

### 4.5 演習4：AIとの協働開発（Level 3-4）

**課題：** AIを使ってミニゲームを作る。因数分解してタスクを分割し、出力を評価・修正。

**学習目標：** 3層モデルの実践、AIとの効果的な協働、判断力

### 4.6 演習5：コードレビュー（Level 3-4）

**課題：** AIが生成したコードをレビュー。バグ、改善点、セキュリティリスクを評価。

**学習目標：** 批判的思考、コード品質評価、セキュリティ意識

---

## 第5章：進路と将来

### 5.1 ゲーム業界への道

- 新卒採用：ポートフォリオ重視
- インターン：実務経験を積む
- 個人開発：作品で証明する

### 5.2 ポートフォリオの作り方

- 完成した作品を並べる
- 制作過程を説明する
- GitHubでコードを公開
- 「見せる」意識を持つ

### 5.3 ゲーム業界以外の選択肢

ゲームAIの知識は汎用的。

- シミュレーション産業
- ロボティクス
- 自動運転
- 研究職

### 5.4 AI時代のキャリア戦略

- AIに代替されにくいスキル
- 判断力、創造性、コミュニケーション
- AIを使いこなす能力
- 変化に適応する力

### 5.5 10年後を想像する

10年前、スマホゲームは主流ではなかった。10年後、何が主流かは分からない。

だから「変化に対応する力」が重要。基礎があれば、変化に対応できる。

---

## 第6章：学習リソース

### 6.1 書籍（プログラミング基礎）

- 「プログラムはなぜ動くのか」矢沢久雄
- 「リーダブルコード」Dustin Boswell
- 「Clean Code」Robert C. Martin

### 6.2 書籍（アルゴリズム）

- 「アルゴリズム図鑑」石田保輝
- 「プログラミングコンテストチャレンジブック」秋葉拓哉
- 「Introduction to Algorithms」CLRS

### 6.3 書籍（ゲームAI）

- 「ゲームAI技術入門」三宅陽一郎
- 「Artificial Intelligence for Games」Ian Millington
- 「AI Game Programming Wisdom」シリーズ

### 6.4 オンラインコース

- Coursera：Machine Learning（Andrew Ng）
- Udemy：Unity、Unreal Engineコース
- YouTube：無料の技術解説動画

### 6.5 コミュニティ

- Discord：Unity Japan、Unreal Engine Japan
- Twitter/X：技術者をフォロー
- 孤独に学ばない

---

# 総括：AI時代に価値を持つ人間とは

---

## AI時代の誤解

「AIが賢くなる＝人間は不要」ではない。

## AIが強くなるほど人間が問われる

判断・責任・意味づけ。これが人間の仕事。

## 3層モデルの再確認

1. **因数分解**：問題を適切な粒度に分解する
2. **明確化**：目的・制約・仮説を言語化する
3. **判断**：選択・優先順位・責任を引き受ける

これが全ての基礎。

## 「使われる側」にならないために

判断を放棄しないこと。

## あなたは何を持ち、何を任せるか

これは技術の話ではなく、生き方の話。

## 作った感を大切にする

成果物に、あなたの意図を残す。

## 失敗を恐れない

失敗は設計を洗練させる。

## 小さく、しかし深く

広く浅くより、狭く深く。

## 継続できるテーマを持つ

AI時代は、継続が最大の差になる。

## 学び続ける人が強い

ツールは変わる。思考は残る。

---

# 最終メッセージ

AIが強くなるほど、人間の価値は消えるのではなく、変質して上がる。

論理と思考、直感と創造性。

両方を持つ人になってください。

---

# 参考文献

## 学術論文

1. Shannon, C. E. (1950). "Programming a Computer for Playing Chess."
2. Hart, P. E., Nilsson, N. J., & Raphael, B. (1968). "A Formal Basis for the Heuristic Determination of Minimum Cost Paths."
3. Knuth, D. E., & Moore, R. W. (1975). "An Analysis of Alpha-Beta Pruning."
4. Vaswani, A., et al. (2017). "Attention Is All You Need."
5. Kaplan, J., et al. (2020). "Scaling Laws for Neural Language Models."
6. Brown, T., et al. (2020). "Language Models are Few-Shot Learners."
7. Wei, J., et al. (2022). "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models."
8. Mnih, V., et al. (2015). "Human-level control through deep reinforcement learning."
9. Silver, D., et al. (2016). "Mastering the game of Go with deep neural networks and tree search."
10. Silver, D., et al. (2017). "Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm."
11. Li, J., et al. (2020). "Suphx: Mastering Mahjong with Deep Reinforcement Learning."
12. Berner, C., et al. (2019). "Dota 2 with Large Scale Deep Reinforcement Learning."
13. Pearce, H., et al. (2022). "Asleep at the Keyboard? Assessing the Security of GitHub Copilot's Code Contributions."
14. Bastani, H., et al. (2024). "Generative AI Can Harm Learning."
15. Kapur, M. (2008). "Productive Failure."
16. Bjork, R. A. (1994). "Memory and Metamemory Considerations in the Training of Human Beings."
17. Roediger, H. L., & Karpicke, J. D. (2006). "Test-Enhanced Learning."

## GDC講演・業界資料

1. Isla, D. (2005). "Handling Complexity in the Halo 2 AI." *GDC 2005*.
2. Orkin, J. (2006). "Three States and a Plan: The A.I. of F.E.A.R." *GDC 2006*.
3. Booth, M. (2009). "The AI Systems of Left 4 Dead." *GDC 2009*.
4. Butcher, C., & Griesemer, J. (2002). "The Illusion of Intelligence: The Integration of AI and Level Design in Halo." *GDC 2002*.
5. Pittman, J. (2011). "The Pac-Man Dossier." *Gamasutra*.

## 書籍

1. Millington, I., & Funge, J. (2009). *Artificial Intelligence for Games*. Morgan Kaufmann.
2. Rabin, S. (Ed.). *AI Game Programming Wisdom* series. Charles River Media.
3. 三宅陽一郎 (2020). 「ゲームAI技術入門」. 技術評論社.

---

**文書情報**

- タイトル：AI技術の活用（ゲームとAI）2026
- 副題：AI時代の「作り方」と人間の役割
- 対象：久留米大学 2026年度講義
- 総ページ数：約80ページ（推定）
- 作成日：2026年1月9日
